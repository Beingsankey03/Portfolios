{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c204eb6e3454acc9949f22039394199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a92e46e639f347e3b312b0f80dedfe65",
              "IPY_MODEL_f63a1fbe231c43ac8f03d2eee9a04f40",
              "IPY_MODEL_ac407172a59140d1b8cbc174414eaef5"
            ],
            "layout": "IPY_MODEL_3d91c647c5254cf892595cad18e2533b"
          }
        },
        "a92e46e639f347e3b312b0f80dedfe65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34a5214215ec44abb9d44db9d091745c",
            "placeholder": "​",
            "style": "IPY_MODEL_a8de1e04655846cf95fc293c8b04b582",
            "value": "100%"
          }
        },
        "f63a1fbe231c43ac8f03d2eee9a04f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8d874f9c8384c95a506dc39da333bfd",
            "max": 6532,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cbd2ea1552e4c04adfe1023aa886328",
            "value": 6532
          }
        },
        "ac407172a59140d1b8cbc174414eaef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcb4c8df7e664aef8668f06ab2bcfbc8",
            "placeholder": "​",
            "style": "IPY_MODEL_a2339d79950047fcaad3245cdf195ec7",
            "value": " 6532/6532 [00:16&lt;00:00, 457.33it/s]"
          }
        },
        "3d91c647c5254cf892595cad18e2533b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a5214215ec44abb9d44db9d091745c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8de1e04655846cf95fc293c8b04b582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8d874f9c8384c95a506dc39da333bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cbd2ea1552e4c04adfe1023aa886328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcb4c8df7e664aef8668f06ab2bcfbc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2339d79950047fcaad3245cdf195ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e304be81955e48d59df2c983a8848ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f62f26f63b549e28c929d4aa07012ac",
              "IPY_MODEL_8b4836bc2ee64586b3de0f5786e92c7a",
              "IPY_MODEL_6ee9b144fbe8412482374daebc2843db"
            ],
            "layout": "IPY_MODEL_e836b8f2d3c64a2c86e201c63c730d61"
          }
        },
        "8f62f26f63b549e28c929d4aa07012ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7893b4cf88334cf194d97371a6fb40b6",
            "placeholder": "​",
            "style": "IPY_MODEL_0e3b13e134524c4d9de9edfd07d2a81b",
            "value": "100%"
          }
        },
        "8b4836bc2ee64586b3de0f5786e92c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ad55c38dce24dd1bd64e748f7f971a0",
            "max": 6532,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fffbaa3bfd114ffc9a0bcf05b5b88cdf",
            "value": 6532
          }
        },
        "6ee9b144fbe8412482374daebc2843db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb6e19d8cffc47398c68cf6f445ff8e2",
            "placeholder": "​",
            "style": "IPY_MODEL_3cbf1ea4813f488cbf32d62d0ad50dd7",
            "value": " 6532/6532 [00:24&lt;00:00, 44.79it/s]"
          }
        },
        "e836b8f2d3c64a2c86e201c63c730d61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7893b4cf88334cf194d97371a6fb40b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3b13e134524c4d9de9edfd07d2a81b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ad55c38dce24dd1bd64e748f7f971a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fffbaa3bfd114ffc9a0bcf05b5b88cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb6e19d8cffc47398c68cf6f445ff8e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cbf1ea4813f488cbf32d62d0ad50dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db198ea769c242d1a69250a3dd106d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d73f1022d4242de96daf12c483c24b9",
              "IPY_MODEL_a43911feefed426db13221bc2fbc51ea",
              "IPY_MODEL_0e574543aaee4fc09c6846996d654d41"
            ],
            "layout": "IPY_MODEL_39fc3485ad394ab48c68e5b141f2261f"
          }
        },
        "9d73f1022d4242de96daf12c483c24b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1806c1f6401b4cbc879fc27cec272f2d",
            "placeholder": "​",
            "style": "IPY_MODEL_a01cd2af4f1743629a4aabf2e7701304",
            "value": "100%"
          }
        },
        "a43911feefed426db13221bc2fbc51ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8960d909a4d424297ff8351cbf9f352",
            "max": 6532,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab776ba2a6e04789a449203503b24f7f",
            "value": 6532
          }
        },
        "0e574543aaee4fc09c6846996d654d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ddba80f1a6a47b5ba2233c99c9d9d45",
            "placeholder": "​",
            "style": "IPY_MODEL_d64cfc1bb74f4fba815966bd0c6db9f4",
            "value": " 6532/6532 [00:16&lt;00:00, 410.61it/s]"
          }
        },
        "39fc3485ad394ab48c68e5b141f2261f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1806c1f6401b4cbc879fc27cec272f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a01cd2af4f1743629a4aabf2e7701304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8960d909a4d424297ff8351cbf9f352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab776ba2a6e04789a449203503b24f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ddba80f1a6a47b5ba2233c99c9d9d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d64cfc1bb74f4fba815966bd0c6db9f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Dependencies"
      ],
      "metadata": {
        "id": "rRZs0rfF1DaO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "xXrC-H7W0UrQ",
        "outputId": "264d1cb3-7bbc-4d78-ee3c-b00020462775"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWhen importing new libraries make sure they are in a packge in ondemand environment or else it won't work\\nLibraries:\\n\\nTextblob (Sentiment Classifier)\\nTweepy (Tweepy Stopwords remover, it won't remove all the punctuations)\\nTensorflow / Pytorch\\nSpacy to get the POS, Named Entity Recognition (To get more features)\\n\\nApproach: Data is unbalanced\\nYou need to balance the data to make the training valid\\n\\nStep 1:\\nData Exploratory:\\nCheck for null values, missing values\\nMake some graph to show the imbalance\\n\\nStep 2:\\nSplit the data into training and validation (80 and 20)\\n\\nStep 3:\\nPreprocessing the data\\n 0) Make into Lowercase\\n 1) Keep the punctuation to test how our model works and vice versa (Try to reomve unneccessary punctuation like ##)\\n 2) Try Stemming and Lemmatization\\n 3) Stopwords can be removed if we are not using contextualized embeddings like BERT\\n 4) Try to remove the numbers it doesn't make sense\\n\\nStep 4:\\nBalancing technique: \\nSMOTE(imblearn) --> geenrate the synthetic data\\npandas Resample - oversample and undersample\\ntry xgboost without oversampling and undersampling\\n\\nStep 5:\\nFeature Engineering\\nIf we are not using the contextualized embeddings we can split into Bigrams and unigrams\\n\\nStep 6:\\nModeling\\nNaive Bayes or Decision Tree or Random Forest (Creating a baseline model)\\nXgboost\\nBERT (if we are not removing the stopwords)\\nTry Neural Network models for classification (eg. LSTM)\\n\\nStep 7:\\nIF time is available try to implement sequential feature selector to remove the unnecessary features (Unigram, Bigram, Trigram, Parts_Of_Speech_Unigram)\\nCalculate the Macro Average as the class is unbalanced and also Macro Precision, Recall and F1 score\\n\\nStep 8:\\nPerform Cross Validation to reduce over fitting of the model and increase the variance of the model\\n\\nStep 9:\\nHypertune the model by finding the right parameters (Grid Search is one way but leads to high time complexity)\\n\\nStep 10:\\nSave the model in a pickle file\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# OUR GENERAL PLAN:\n",
        "\"\"\"\n",
        "When importing new libraries make sure they are in a packge in ondemand environment or else it won't work\n",
        "Libraries:\n",
        "\n",
        "Textblob (Sentiment Classifier)\n",
        "Tweepy (Tweepy Stopwords remover, it won't remove all the punctuations)\n",
        "Tensorflow / Pytorch\n",
        "Spacy to get the POS, Named Entity Recognition (To get more features)\n",
        "\n",
        "Approach: Data is unbalanced\n",
        "You need to balance the data to make the training valid\n",
        "\n",
        "Step 1:\n",
        "Data Exploratory:\n",
        "Check for null values, missing values\n",
        "Make some graph to show the imbalance\n",
        "\n",
        "Step 2:\n",
        "Split the data into training and validation (80 and 20)\n",
        "\n",
        "Step 3:\n",
        "Preprocessing the data\n",
        " 0) Make into Lowercase\n",
        " 1) Keep the punctuation to test how our model works and vice versa (Try to reomve unneccessary punctuation like ##)\n",
        " 2) Try Stemming and Lemmatization\n",
        " 3) Stopwords can be removed if we are not using contextualized embeddings like BERT\n",
        " 4) Try to remove the numbers it doesn't make sense\n",
        "\n",
        "Step 4:\n",
        "Balancing technique:\n",
        "SMOTE(imblearn) --> geenrate the synthetic data\n",
        "pandas Resample - oversample and undersample\n",
        "try xgboost without oversampling and undersampling\n",
        "\n",
        "Step 5:\n",
        "Feature Engineering\n",
        "If we are not using the contextualized embeddings we can split into Bigrams and unigrams\n",
        "\n",
        "Step 6:\n",
        "Modeling\n",
        "Naive Bayes or Decision Tree or Random Forest (Creating a baseline model)\n",
        "Xgboost\n",
        "BERT (if we are not removing the stopwords)\n",
        "Try Neural Network models for classification (eg. LSTM)\n",
        "\n",
        "Step 7:\n",
        "IF time is available try to implement sequential feature selector to remove the unnecessary features (Unigram, Bigram, Trigram, Parts_Of_Speech_Unigram)\n",
        "Calculate the Macro Average as the class is unbalanced and also Macro Precision, Recall and F1 score\n",
        "\n",
        "Step 8:\n",
        "Perform Cross Validation to reduce over fitting of the model and increase the variance of the model\n",
        "\n",
        "Step 9:\n",
        "Hypertune the model by finding the right parameters (Grid Search is one way but leads to high time complexity)\n",
        "\n",
        "Step 10:\n",
        "Save the model in a pickle file\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Dataset"
      ],
      "metadata": {
        "id": "0W92iy8Q1Hky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at file length, value counts of emotions to see how to balance\n",
        "full_file = pd.read_csv('/content/sample_data/T1_train.csv')\n",
        "print(full_file['label'].value_counts())\n",
        "full_file_len = len(full_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTAHF4lc09_g",
        "outputId": "a6d1241b-ec28-4a7c-fdc8-1f9568643faa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral       2616\n",
            "gratitude     1255\n",
            "admiration     818\n",
            "love           458\n",
            "amusement      388\n",
            "curiosity      251\n",
            "approval       236\n",
            "anger          177\n",
            "optimism       175\n",
            "sadness        158\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "_wIfB3mO1gr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntV4jU3V5sIg",
        "outputId": "b6396aa9-6f25-4636-9256-0a30292fe649"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh2eZW446k6M",
        "outputId": "469f026e-51f9-438e-a6e0-40a574e16db6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqJMBtZD6vji",
        "outputId": "3b169670-a5f2-43b5-b51e-5b533ab9a15c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Step 3:\n",
        "Preprocessing the data\n",
        " 0) Make into Lowercase\n",
        " 1) Keep the punctuation to test how our model works and vice versa (Try to reomve unneccessary punctuation like ##)\n",
        " 2) Try Stemming and Lemmatization\n",
        " 3) Stopwords can be removed if we are not using contextualized embeddings like BERT\n",
        " 4) Try to remove the numbers it doesn't make sense\n",
        "\n",
        "\"\"\"\n",
        "def preprocess_each_line(corpus_df_text_line, corpus_df):\n",
        "  # THIS APPORACH HARDER TO FACTOR IN EMOTION OF NEUTRAL, HAVE TO GO BACK TO DO\n",
        "  ### print(corpus_df_text_line)\n",
        "  new_text_line = \"\"\n",
        "  tag_map = defaultdict(lambda : wordnet.NOUN)\n",
        "  tag_map['J'] = wordnet.ADJ\n",
        "  tag_map['V'] = wordnet.VERB\n",
        "  tag_map['R'] = wordnet.ADV\n",
        "  new_corpus_line = corpus_df_text_line\n",
        "\n",
        "  # lower case\n",
        "  new_corpus_line = new_corpus_line.lower()\n",
        "\n",
        "  # remove numbers\n",
        "  new_corpus_line = re.sub(r'[0-9]', '', new_corpus_line)\n",
        "\n",
        "  # Remove [NAME] token manually\n",
        "  new_corpus_line = new_corpus_line.replace(\"[name]\", '')\n",
        "\n",
        "  # Remove punc\n",
        "  new_corpus_line = re.sub(r'[^\\w\\s]', '', new_corpus_line)\n",
        "\n",
        "  # tokenize with tweet tokenizer (keeps contractions not split to remove later)\n",
        "  tokenizer = TweetTokenizer()\n",
        "  new_corpus_line = tokenizer.tokenize(new_corpus_line)\n",
        "  # Filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  new_corpus_line = [word for word in new_corpus_line if word not in stopwords.words('english')]\n",
        "\n",
        "  # Lemantize with word net\n",
        "\n",
        "  lmtzr = WordNetLemmatizer()\n",
        "  for token, tag in nltk.pos_tag(new_corpus_line):\n",
        "      lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "      #print(token, \"=>\", lemma)\n",
        "      if token != lemma:\n",
        "          if token in new_corpus_line:\n",
        "              index = new_corpus_line.index(token)\n",
        "              new_corpus_line[index] = lemma\n",
        "    # Final filter for bad punctuation\n",
        "  bad_punc = [',','','.','-', '\\'', '\"', '’', '”', '“', \">\", '`', ''] #convert to regex when not mad at regex\n",
        "  new_corpus_line = [token for token in new_corpus_line if token not in bad_punc]\n",
        "  ### print(new_corpus_line)\n",
        "\n",
        "  new_corpus_line = ' '.join(new_corpus_line)\n",
        "  return new_corpus_line\n",
        "def preprocess(corpus_df, exclude_neutral = False):\n",
        "  #tqdm.pandas()\n",
        "  running_count = 0\n",
        "  tqdm.pandas()\n",
        "  print('[PREPROCESSING] starting...')\n",
        "  print('[PREPROCESSING] set before preprocessing:')\n",
        "  print(corpus_df['text'])\n",
        "  print(corpus_df['label'].value_counts())\n",
        "  print(f'[PREPROCESSING] starting preprocess on set of size {len(corpus_df)}')\n",
        "  corpus_df['text'] = corpus_df['text'].progress_apply(lambda x: preprocess_each_line(x, corpus_df))\n",
        "  corpus_df = corpus_df[corpus_df['text'].str.len() > 0]\n",
        "  corpus_df.reset_index(inplace=True)\n",
        "  print('[PREPROCESSING] done!')\n",
        "  # MAYBE REMOVE?\n",
        "  #corpus_df.drop_duplicates()\n",
        "  print('[PREPROCESSING] set after process:')\n",
        "  print(corpus_df['text'])\n",
        "  print(corpus_df['label'].value_counts())\n",
        "  if exclude_neutral:\n",
        "    corpus_df = corpus_df[corpus_df[\"label\"].str.contains(\"neutral\")==False]\n",
        "  return corpus_df\n",
        "\n",
        "\n",
        "# Demonstration of preprocessing working\n",
        "temp_whole_corpus = pd.read_csv('/content/sample_data/T1_train.csv')\n",
        "print(type(temp_whole_corpus))\n",
        "\n",
        "print(temp_whole_corpus)\n",
        "preprocessed_whole_corpus = preprocess(temp_whole_corpus)\n",
        "print(preprocessed_whole_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8c204eb6e3454acc9949f22039394199",
            "a92e46e639f347e3b312b0f80dedfe65",
            "f63a1fbe231c43ac8f03d2eee9a04f40",
            "ac407172a59140d1b8cbc174414eaef5",
            "3d91c647c5254cf892595cad18e2533b",
            "34a5214215ec44abb9d44db9d091745c",
            "a8de1e04655846cf95fc293c8b04b582",
            "e8d874f9c8384c95a506dc39da333bfd",
            "3cbd2ea1552e4c04adfe1023aa886328",
            "dcb4c8df7e664aef8668f06ab2bcfbc8",
            "a2339d79950047fcaad3245cdf195ec7"
          ]
        },
        "id": "JWZQcUqg1NYa",
        "outputId": "6fdde437-6d48-45c4-b43b-98cd44e2ff44"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "                                                   text       label\n",
            "0                        Thank you all for the insight.   gratitude\n",
            "1     I am already using ublock. Thank you for sugge...   gratitude\n",
            "2                                       That’s awesome!  admiration\n",
            "3                                               True :/     neutral\n",
            "4              Also likely selling stolen goods too lol   amusement\n",
            "...                                                 ...         ...\n",
            "6527     Knicks fans chanting lets go heat is sickening     neutral\n",
            "6528                                         Goddam it!       anger\n",
            "6529  Those charts seem to indicate approximately a ...     neutral\n",
            "6530                                   That'll cost ya.     neutral\n",
            "6531    Question for [NAME] the incentive to work hard?   curiosity\n",
            "\n",
            "[6532 rows x 2 columns]\n",
            "[PREPROCESSING] starting...\n",
            "[PREPROCESSING] set before preprocessing:\n",
            "0                          Thank you all for the insight.\n",
            "1       I am already using ublock. Thank you for sugge...\n",
            "2                                         That’s awesome!\n",
            "3                                                 True :/\n",
            "4                Also likely selling stolen goods too lol\n",
            "                              ...                        \n",
            "6527       Knicks fans chanting lets go heat is sickening\n",
            "6528                                           Goddam it!\n",
            "6529    Those charts seem to indicate approximately a ...\n",
            "6530                                     That'll cost ya.\n",
            "6531      Question for [NAME] the incentive to work hard?\n",
            "Name: text, Length: 6532, dtype: object\n",
            "neutral       2616\n",
            "gratitude     1255\n",
            "admiration     818\n",
            "love           458\n",
            "amusement      388\n",
            "curiosity      251\n",
            "approval       236\n",
            "anger          177\n",
            "optimism       175\n",
            "sadness        158\n",
            "Name: label, dtype: int64\n",
            "[PREPROCESSING] starting preprocess on set of size 6532\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6532 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c204eb6e3454acc9949f22039394199"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PREPROCESSING] done!\n",
            "[PREPROCESSING] set after process:\n",
            "0                                           thank insight\n",
            "1       already use ublock thank suggest ghostery didn...\n",
            "2                                           thats awesome\n",
            "3                                                    true\n",
            "4                         also likely sell steal good lol\n",
            "                              ...                        \n",
            "6485               knicks fan chant let go heat sickening\n",
            "6486                                               goddam\n",
            "6487    chart seem indicate approximately net advantag...\n",
            "6488                                       thatll cost ya\n",
            "6489                         question incentive work hard\n",
            "Name: text, Length: 6490, dtype: object\n",
            "neutral       2576\n",
            "gratitude     1255\n",
            "admiration     818\n",
            "love           458\n",
            "amusement      388\n",
            "curiosity      250\n",
            "approval       236\n",
            "anger          176\n",
            "optimism       175\n",
            "sadness        158\n",
            "Name: label, dtype: int64\n",
            "      index                                               text       label\n",
            "0         0                                      thank insight   gratitude\n",
            "1         1  already use ublock thank suggest ghostery didn...   gratitude\n",
            "2         2                                      thats awesome  admiration\n",
            "3         3                                               true     neutral\n",
            "4         4                    also likely sell steal good lol   amusement\n",
            "...     ...                                                ...         ...\n",
            "6485   6527             knicks fan chant let go heat sickening     neutral\n",
            "6486   6528                                             goddam       anger\n",
            "6487   6529  chart seem indicate approximately net advantag...     neutral\n",
            "6488   6530                                     thatll cost ya     neutral\n",
            "6489   6531                       question incentive work hard   curiosity\n",
            "\n",
            "[6490 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spitting of Test and Train"
      ],
      "metadata": {
        "id": "h8Iw13bO7E_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# demonstration function of how to split set, in actual implementation just calling train_test_split\n",
        "def split_set(file_to_split):\n",
        "    print(f'[SPLITTING SET]')\n",
        "    full_data = pd.read_csv(file_to_split)\n",
        "\n",
        "    all_emotionlabels = full_data['label'] # x\n",
        "    all_texts = full_data['text'] # y\n",
        "\n",
        "    emotionlabels_train, emotionlabels_test, texts_train, texts_test = train_test_split(all_emotionlabels, all_texts, train_size=.8, shuffle=False)\n",
        "\n",
        "    return emotionlabels_train, texts_train, emotionlabels_test, texts_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "emotionlabels_train, texts_train, emotionlabels_test, texts_test = split_set('/content/sample_data/T1_train.csv')\n",
        "print(\"------------------ TRAIN SET ------------------\")\n",
        "\n",
        "print(emotionlabels_train.value_counts())\n",
        "print(texts_train)\n",
        "print(\"------------------ TEST SET ------------------\")\n",
        "print(emotionlabels_test.value_counts())\n",
        "print(texts_test)\n",
        "\n",
        "\"\"\"\n",
        "print(len(train_set['label']))\n",
        "print(train_set['label'].value_counts())\n",
        "print(len(test_set['label']))\n",
        "print(test_set['label'].value_counts())\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "wPJbz6ru7Ejs",
        "outputId": "04aa2614-a3ad-4b74-f651-153d5f9377ca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SPLITTING SET]\n",
            "------------------ TRAIN SET ------------------\n",
            "neutral       2092\n",
            "gratitude     1025\n",
            "admiration     645\n",
            "love           352\n",
            "amusement      314\n",
            "curiosity      205\n",
            "approval       186\n",
            "optimism       144\n",
            "anger          142\n",
            "sadness        120\n",
            "Name: label, dtype: int64\n",
            "0                          Thank you all for the insight.\n",
            "1       I am already using ublock. Thank you for sugge...\n",
            "2                                         That’s awesome!\n",
            "3                                                 True :/\n",
            "4                Also likely selling stolen goods too lol\n",
            "                              ...                        \n",
            "5220    She looks like her face is pulled back with ''...\n",
            "5221                 i really hope this is old and ironic\n",
            "5222    Everyone started making remakes. Here is my fa...\n",
            "5223       love ya bro, i just love how positive you are!\n",
            "5224    Thanks for sharing. They also have an Amazon w...\n",
            "Name: text, Length: 5225, dtype: object\n",
            "------------------ TEST SET ------------------\n",
            "neutral       524\n",
            "gratitude     230\n",
            "admiration    173\n",
            "love          106\n",
            "amusement      74\n",
            "approval       50\n",
            "curiosity      46\n",
            "sadness        38\n",
            "anger          35\n",
            "optimism       31\n",
            "Name: label, dtype: int64\n",
            "5225    Appreciate the reply, I'll look into these. Th...\n",
            "5226                                          u fucked up\n",
            "5227                   oh the new Zealand thing thank you\n",
            "5228             Amazing bot, you’re doing great sweetie \n",
            "5229                                      Beard length 10\n",
            "                              ...                        \n",
            "6527       Knicks fans chanting lets go heat is sickening\n",
            "6528                                           Goddam it!\n",
            "6529    Those charts seem to indicate approximately a ...\n",
            "6530                                     That'll cost ya.\n",
            "6531      Question for [NAME] the incentive to work hard?\n",
            "Name: text, Length: 1307, dtype: object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprint(len(train_set['label']))\\nprint(train_set['label'].value_counts())\\nprint(len(test_set['label']))\\nprint(test_set['label'].value_counts())\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMOTION CLASSIFIER"
      ],
      "metadata": {
        "id": "VZvSBUUr7eD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################### PS1 : EmotionClassifier (task 1.1-1.2) #####################\n",
        "# Created by Kenyon, Vrushabh, Ketaki, Shrinivas\n",
        "# FINAL SYSTEM -- This can be put into a python file and run as imported object\n",
        "# LAST CHANGE FROM GOOGLE COLAB: 1:46 pm 10/25/22import os\n",
        "##################################################################################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "# ----------------------- HELPER FUNCTIONS -------------------------------\n",
        "\n",
        "def pickle_object_save(object, filename):\n",
        "    # Function to save entire classifier object as pickle file\n",
        "  with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
        "    pickle.dump(object, outp, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "'''\n",
        "def pretty_print(self,type_info,location, information):\n",
        "    # function to indent and print in nice way, just for formatting/consistency\n",
        "    print_str = f'[EC - {type_info}:{location}]'\n",
        "    padlen = 25\n",
        "    for counter in range(0, padlen-len(print_str)):\n",
        "      print_str += ' '\n",
        "    print_str += f'--: {information}'\n",
        "    print(print_str)\n",
        "'''\n",
        "\n",
        "\n",
        "# --------------------- EMOTIONCLASSIFIER Object -------------------------------------\n",
        "class EmotionClassifier:\n",
        "    # MAIN emotion classifier object\n",
        "    # Functions:\n",
        "    #     init: initalize object\n",
        "    #     prettyprint: formatting pretty function\n",
        "    #     preprocess_test_corpus: function to take in csv file name, and output preprocessed dataframe\n",
        "    #     test_model: function to take generated model and use it to predict on test\n",
        "    #     print_training_stats: output function to print results from saved parameters\n",
        "    #     oversample: function to oversample a df given a df (balancing)\n",
        "    #     [NOT USED] generate_prediction_classreport : function to generate classreport from metrics library\n",
        "    #     [NOT USED] generate_acc_wcv: function to use cross validation (5 fold) to generate accuracy\n",
        "    #     train_model: main function to train and generate model using linear SVC given a preprocessed corpus\n",
        "    #     predict_emotion: helper function to predict given trained model and a text\n",
        "    #     validate_train: function to validate training model based on using proportion of train to test\n",
        "    #     preprocess: MAIN preprocessing function runner, will loop through each line of corpus and preprocess\n",
        "    #     preprocess_each_line : main preprocessing subfunction, will take a line and output preprocessed version\n",
        "\n",
        "    # Sources:\n",
        "    #         https://www.analyticsvidhya.com/blog/2021/11/a-guide-to-building-an-end-to-end-multiclass-text-classification-model/\n",
        "    #         ttps://stackoverflow.com/questions/34714162/preventing-splitting-at-apostrophies-when-tokenizing-words-using-nltk\n",
        "    #         https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "    #         https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-multilabel-svm-classifier-with-scikit-learn.md\n",
        "    #         https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
        "    #         https://medium.com/analytics-vidhya/undersampling-and-oversampling-an-old-and-a-new-approach-4f984a0e8392\n",
        "\n",
        "  def pretty_print(self,type_info,location, information):\n",
        "    # function to indent and print in nice way, just for formatting/consistency\n",
        "    print_str = f'[EC - {type_info}:{location}]'\n",
        "    padlen = 25\n",
        "    for counter in range(0, padlen-len(print_str)):\n",
        "        print_str += ' '\n",
        "        print_str += f'--: {information}'\n",
        "        print(print_str)\n",
        "  def __init__(self, train_file_name, test_file_name, test_file = False, exclude_neutral=False):\n",
        "\n",
        "    # init: initalization function for obj\n",
        "    #      train_file_name (csv file for train)\n",
        "    #      test_file_name (usally empty, but filename if recieved)\n",
        "    #      test_file = false (simple boolean statement to see if test file)\n",
        "    #      exclude_neutral (parameter to generate train based on neutrals, defaults to false)\n",
        "    #   Init will preprocess the sets to then be ready to call train function\n",
        "\n",
        "    self.test_models = [\n",
        "        # All models we were testing, used LinearSVC in final model, this is redundant and not actually used\n",
        "        RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0, class_weight=\"balanced\"),\n",
        "        LinearSVC(),\n",
        "        MultinomialNB(),\n",
        "        LogisticRegression(random_state=0),\n",
        "    ]\n",
        "\n",
        "    self.pretty_print('START', 'INIT', 'Initalizing Emotion Classifier...')\n",
        "    self.pretty_print('NLTK', 'INIT', 'Downloading NLTK Data...')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    time.sleep(1)\n",
        "    self.train_corpus = pd.read_csv(train_file_name)\n",
        "    self.test_corpus = test_file_name # will be given this later\n",
        "    self.pretty_print('NLTK', 'INIT', 'NLTK Data Download Complete!')\n",
        "    if exclude_neutral:\n",
        "      self.pretty_print('INFO', 'INIT', 'Preforming Pre-processing on train corpus (w/o neutral labels)...')\n",
        "\n",
        "      self.train_corpus_preproccessed = self.preprocess(self.train_corpus, exclude_neutral=True)\n",
        "    else:\n",
        "      self.pretty_print('INFO', 'INIT', 'Preforming Pre-processing on train corpus...')\n",
        "      self.train_corpus_preproccessed = self.preprocess(self.train_corpus)\n",
        "\n",
        "    # Initalize Parameters\n",
        "\n",
        "    self.trained_model = None\n",
        "    self.model_acc_info = None\n",
        "    self.model_acc_train = None\n",
        "    self.model_acc_predict = None\n",
        "    self.model_classreport_train = None\n",
        "    self.model_classreport_predict = None\n",
        "    self.model_prediction_results = None\n",
        "    self.train_model_fitted_vectorizer = None\n",
        "    self.train_model_x_validate = None\n",
        "    self.train_model_y_validate = None\n",
        "    self.train_model_x_train = None\n",
        "    self.train_model_y_train = None\n",
        "    self.model_validation_predictions = None\n",
        "\n",
        "    self.test_corpus = None\n",
        "    self.test_packed_results = None\n",
        "\n",
        "    self.pretty_print('DONE', 'INIT', 'Initalization complete')\n",
        "\n",
        "\n",
        "  def preprocess_test_corpus(self, test_filename, exclude_neutral = False):\n",
        "    # preprocess_test_corpus: function to take in csv file name, and output preprocessed dataframe\n",
        "    #      test_filename : filename of test set\n",
        "    #      exclude_neutral : parameter to include neutral in test set or not\n",
        "    #\n",
        "\n",
        "    self.test_corpus = self.preprocess(pd.read_csv(test_filename), exclude_neutral)\n",
        "\n",
        "\n",
        "\n",
        "  def test_model(self):\n",
        "    # test_model: function to take generated model and use it to predict on test\n",
        "    #      self (grabs parameters from obj)\n",
        "    #   will print out resutls including accuracy, macro acc, recall, precision, f1\n",
        "    self.pretty_print('START', 'TEST_MODEL', 'Starting Test')\n",
        "    prediction_entries = []\n",
        "    predicted_labels = []\n",
        "    for (text, emotionlabel) in itertools.zip_longest(self.test_corpus['text'], self.test_corpus['label']):\n",
        "      labeled_emotion = emotionlabel\n",
        "      predicted_emotion = self.predict_emotion(text)\n",
        "      result = (text, labeled_emotion, predicted_emotion[0])\n",
        "      predicted_labels.append(predicted_emotion[0])\n",
        "      prediction_entries.append(result)\n",
        "    soft_count = 0\n",
        "    err_count = 0\n",
        "    for each in prediction_entries:\n",
        "      soft_count +=1\n",
        "      #print(each)\n",
        "      if each[1] != each[2]:\n",
        "        err_count +=1\n",
        "    accuracy = str((1-(err_count/soft_count)) * 100)\n",
        "    #self.model_prediction_results = [soft_count, err_count, accuracy]\n",
        "    self.pretty_print('DONE', 'TEST_MODEL', 'Testing Complete!')\n",
        "\n",
        "    self.pretty_print('INFO', 'TEST_MODEL', 'Results from classifier')\n",
        "    print(f'\\t\\t TOTAL PREDICTIONS: {soft_count}')\n",
        "    print(f'\\t\\t INCORRECT PREDICTIONS: {err_count}')\n",
        "    print(f'\\t\\t OVERALL ACCURACY: {accuracy}')\n",
        "    class_report_predict = metrics.classification_report(self.test_corpus['label'], predicted_labels, target_names= self.test_corpus['label'].unique())\n",
        "    print(class_report_predict)\n",
        "    self.test_packed_results = [soft_count, err_count, accuracy, class_report_predict]\n",
        "\n",
        "\n",
        "\n",
        "  def print_training_stats(self):\n",
        "    # print_training_stats: output function to print results from saved parameters\n",
        "    #       self: grab vars from object to use\n",
        "    #    will output info on sets before and after, and validation using 20% of train set as test set\n",
        "    self.pretty_print('START', 'TRAININGSTATS', 'Printing Training Stats')\n",
        "    print(\"\\t\\t -- Preprocessed Training Corpus --\")\n",
        "    print(self.train_corpus_preproccessed)\n",
        "    print('\\n')\n",
        "    print(\"\\t\\t -- X_train, Y_train --\")\n",
        "    print(self.train_model_x_train)\n",
        "    print(self.train_model_y_train)\n",
        "    print(\"\\n\")\n",
        "    print(\"\\t\\t -- x_validate, y_validate --\")\n",
        "    print(self.train_model_x_validate)\n",
        "    print(self.train_model_y_validate)\n",
        "    print(\"\\t\\t -- Validation Results (Class report, Accuracy, etc) --\")\n",
        "    print(self.model_classreport_predict)\n",
        "    print (f'\\t Total Classifications durring validation : {self.model_prediction_results[0]}')\n",
        "    print (f'\\t Incorrect Classifications durring validation : {self.model_prediction_results[1]}')\n",
        "    print (f'\\t Total (basic) Classification Accuracy: {self.model_prediction_results[2]}')\n",
        "    print(f'\\t 3 examples of classified data results (text, predicted label, actual label):')\n",
        "    for counter in range(0,3):\n",
        "      print('\\t ', end='')\n",
        "      print(f'{random.choice(self.model_validation_predictions)} \\n')\n",
        "    self.pretty_print('DONE', 'TRAININGSTATS', 'Done with training stat output')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def oversample(self,df):\n",
        "      # oversample: function to oversample a df given a df (balancing)\n",
        "      #      df: dataframe read from csv, will be preprocessed\n",
        "      #   outputs a dataframe upsampled\n",
        "      self.pretty_print('START', 'OVERSAMPLER', 'Starting Oversampling...')\n",
        "      classes = df.label.value_counts().to_dict()\n",
        "      most = max(classes.values())\n",
        "      classes_list = []\n",
        "      for key in classes:\n",
        "          classes_list.append(df[df['label'] == key])\n",
        "      classes_sample = []\n",
        "      for i in range(1,len(classes_list)):\n",
        "          classes_sample.append(classes_list[i].sample(most, replace=True))\n",
        "      df_maybe = pd.concat(classes_sample)\n",
        "      final_df = pd.concat([df_maybe,classes_list[0]], axis=0)\n",
        "      final_df = final_df.reset_index(drop=True)\n",
        "      #print(final_df.label.value_counts())\n",
        "      self.pretty_print('DONE', 'OVERSAMPLER', 'Oversampler complete!')\n",
        "      return final_df\n",
        "\n",
        "  def generate_prediction_classreport(self, features, labels, df_preprocessed_corpus):\n",
        "    # generate_prediction_classreport : function to generate classreport from metrics library\n",
        "    #      features : features generated in train func, pandas series\n",
        "    #      labels : labels generated in train func, pandas series\n",
        "    #      df_preprocessed_corpus : entire preprocessed corpus\n",
        "    #    will set model_classreport_train var to class report -> FUNCTION NOT REALLY NEEDED IN END IMPLEMENTATION\n",
        "    # info on training accuracy\n",
        "\n",
        "    X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, labels, df_preprocessed_corpus.index, test_size=0.25, random_state=1)\n",
        "    model = LinearSVC()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    self.model_classreport_train = metrics.classification_report(y_test, y_pred, target_names= df_preprocessed_corpus['label'].unique())\n",
        "\n",
        "\n",
        "  def generate_acc_wcv(self, model, features, labels):\n",
        "    # generate_acc_wcv: function to use cross validation (5 fold) to generate accuracy\n",
        "    #      model: model being used for stats\n",
        "    #      features : features of model\n",
        "    #      labels : labels of model\n",
        "    #   outputs acc, which is accuracy dataframe containing cross_val_score -> not really used in end emp\n",
        "    #print(features)\n",
        "    #print(labels)\n",
        "    CV = 5\n",
        "    cv_df = pd.DataFrame(index=range(CV * len(self.test_models)))\n",
        "    entries = []\n",
        "    self.pretty_print('INFO', 'TRAINING', 'Gathering Metrics')\n",
        "\n",
        "    print(model)\n",
        "    model_name = model.__class__.__name__\n",
        "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
        "    for fold_idx, accuracy in enumerate(accuracies):\n",
        "      entries.append((model_name, fold_idx, accuracy))\n",
        "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
        "    mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
        "    std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
        "\n",
        "    acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, ignore_index=True)\n",
        "    acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
        "    return acc\n",
        "\n",
        "  def train_model(self, df_preprocessed_corpus):\n",
        "    # train_model: main function to train and generate model using linear SVC given a preprocessed corpus\n",
        "    #      df_preprocessed_corpus : corpus with proeprocessing done by preprocess func\n",
        "    #   function will vectorize with tfidf to get features and labels\n",
        "    #   will then split into train and test (which is really validate)\n",
        "    #   function then oversamples using oversample func\n",
        "    #   function will then generate some metrics and then finally create comppete model using tfidf vectorizer and fitting it\n",
        "\n",
        "\n",
        "    self.pretty_print('START', 'TRAINING', 'Starting training of model')\n",
        "\n",
        "    self.pretty_print('INFO', 'TRAINING', 'Vectorizing features w/tf-idf')\n",
        "    df_preprocessed_corpus['length_after_cleaning'] = df_preprocessed_corpus['text'].apply(lambda x: len(x))\n",
        "    df_preprocessed_corpus['label_id'] = df_preprocessed_corpus['label'].factorize()[0]\n",
        "    label_id_df = df_preprocessed_corpus[['label', 'label_id']].drop_duplicates()\n",
        "    label_to_id = dict(label_id_df.values)\n",
        "    id_to_label = dict(label_id_df[['label_id', 'label']].values)\n",
        "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, ngram_range=(1, 2))\n",
        "    features = tfidf.fit_transform(df_preprocessed_corpus.text).toarray()\n",
        "    labels = df_preprocessed_corpus.label_id\n",
        "    X = df_preprocessed_corpus['text'] # Collection of documents\n",
        "    y = df_preprocessed_corpus['label'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n",
        "    self.pretty_print('INFO', 'TRAINING', 'Splitting Train set into train and validate')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n",
        "\n",
        "    data = {'text':X_train, 'label': y_train}\n",
        "    oversampled = self.oversample(pd.DataFrame(data))\n",
        "    X_train = oversampled['text']\n",
        "    y_train = oversampled['label']\n",
        "    self.train_model_x_train = X_train\n",
        "    self.train_model_y_train = y_train\n",
        "\n",
        "    self.model_acc_train = self.generate_acc_wcv(LinearSVC(), features, labels)\n",
        "    self.pretty_print('INFO', 'TRAINING', 'Generating Model')\n",
        "\n",
        "    self.generate_prediction_classreport(features, labels, df_preprocessed_corpus)\n",
        "\n",
        "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n",
        "    self.pretty_print('INFO', 'TRAINING', 'Model Generation Finished and classreport generated')\n",
        "    fitted_vectorizer = tfidf.fit(X_train)\n",
        "    tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
        "    model = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)\n",
        "\n",
        "    self.train_model_fitted_vectorizer = fitted_vectorizer\n",
        "    self.train_model_x_validate = X_test\n",
        "    self.train_model_y_validate = y_test\n",
        "    self.train_model = model\n",
        "\n",
        "    self.pretty_print('DONE', 'TRAINING', 'Training complete!')\n",
        "\n",
        "\n",
        "  def predict_emotion(self, text):\n",
        "    # predict_emotion: helper function to predict given trained model and a text\n",
        "    #      text: a text that needs to be predicted\n",
        "    #   will output a result in the fourm of array of single emotion val\n",
        "    # helper function which can be called seperately\n",
        "    return self.train_model.predict(self.train_model_fitted_vectorizer.transform([text]))\n",
        "\n",
        "  def validate_train(self):\n",
        "    # validate_train: function to validate training model based on using proportion of train to test\n",
        "    #                 very similar to the test_model function, in all honestly could be same func if not lazy\n",
        "    #       self: parameters generated from other functions, calling own vars using self\n",
        "    #    function will iterate through each text and emotion in validation set\n",
        "    #    and then predict based on line given, and append to all results\n",
        "    #    metrics are then simply generated by counting if predicted label matches given golden label\n",
        "    #    and are set to object vars using self call\n",
        "    self.pretty_print('START', 'VALIDATE_TRAIN', 'Starting Validation based on using 20% of train corpus')\n",
        "    entries = []\n",
        "    predicted_labels = []\n",
        "    for (text, emotionlabel) in itertools.zip_longest(self.train_model_x_validate, self.train_model_y_validate):\n",
        "\n",
        "      labeled_emotion = emotionlabel\n",
        "      predicted_emotion = self.predict_emotion(text)\n",
        "\n",
        "      result = (text, labeled_emotion, predicted_emotion[0])\n",
        "      predicted_labels.append(predicted_emotion[0])\n",
        "\n",
        "      entries.append(result)\n",
        "    self.model_validation_predictions = entries\n",
        "    soft_count = 0\n",
        "    err_count = 0\n",
        "    for each in entries:\n",
        "      soft_count +=1\n",
        "      #print(each)\n",
        "      if each[1] != each[2]:\n",
        "        err_count +=1\n",
        "\n",
        "    accuracy = str((1-(err_count/soft_count)) * 100)\n",
        "    self.model_prediction_results = [soft_count, err_count, accuracy]\n",
        "    #pretty_print('INFO', 'VALIDATE_TRAIN', 'Results from classifier')\n",
        "    #print(f'\\t\\t TOTAL PREDICTIONS: {soft_count}')\n",
        "    #print(f'\\t\\t INCORRECT PREDICTIONS: {err_count}')\n",
        "    #print(f'\\t\\t OVERALL ACCURACY: {accuracy}')\n",
        "    class_report_predict = metrics.classification_report(self.train_model_y_validate, predicted_labels, target_names= self.train_corpus_preproccessed['label'].unique())\n",
        "    self.model_classreport_predict = class_report_predict\n",
        "    #print(class_report_predict)\n",
        "    self.pretty_print('DONE', 'VALIDATE_TRAIN', 'Validation Complete!')\n",
        "\n",
        "  def preprocess(self, corpus_df, exclude_neutral = False):\n",
        "    # preprocess: MAIN preprocessing function runner, will loop through each line of corpus and preprocess\n",
        "    #      corpus_df: entire corpus not preprocessed\n",
        "    #      exclude_neutral: parameter to preprocess and include or exclude neutral labels\n",
        "    #   calls subfunction preprocess_each_line which preforms actual preprocessing, more of runner func\n",
        "    #\n",
        "    def preprocess_each_line(corpus_df_text_line, corpus_df):\n",
        "      # preprocess_each_line : main preprocessing subfunction, will take a line and output preprocessed version\n",
        "      #      corpus_df_text_line: line in corpus to be preprocessed\n",
        "      #      corpus_df: entire corpus df read from pandas, used to get size and when removing interally in this func (now external)\n",
        "      #   function preprocesses by: case folding, removing numerals, removing [name] token entirely, remove any punctuation,\n",
        "      #   tokenize with TweetTokenizer, filter out stop words, lemantize each with wordnet, and remove left over punc chars\n",
        "\n",
        "      new_text_line = \"\"\n",
        "      tag_map = defaultdict(lambda : wordnet.NOUN)\n",
        "      tag_map['J'] = wordnet.ADJ\n",
        "      tag_map['V'] = wordnet.VERB\n",
        "      tag_map['R'] = wordnet.ADV\n",
        "      new_corpus_line = corpus_df_text_line\n",
        "\n",
        "      # lower case\n",
        "      new_corpus_line = new_corpus_line.lower()\n",
        "\n",
        "      # remove numbers\n",
        "      new_corpus_line = re.sub(r'[0-9]', '', new_corpus_line)\n",
        "\n",
        "      # Remove [NAME] token manually\n",
        "      new_corpus_line = new_corpus_line.replace(\"[name]\", '')\n",
        "\n",
        "      # Remove punc\n",
        "      new_corpus_line = re.sub(r'[^\\w\\s]', '', new_corpus_line)\n",
        "\n",
        "      # tokenize with tweet tokenizer (keeps contractions not split to remove later)\n",
        "      tokenizer = TweetTokenizer()\n",
        "      new_corpus_line = tokenizer.tokenize(new_corpus_line)\n",
        "      # Filter out stop words\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      new_corpus_line = [word for word in new_corpus_line if word not in stopwords.words('english')]\n",
        "\n",
        "      # Lemantize with word net\n",
        "\n",
        "      lmtzr = WordNetLemmatizer()\n",
        "      for token, tag in nltk.pos_tag(new_corpus_line):\n",
        "          lemma = lmtzr.lemmatize(token, tag_map[tag[0]])\n",
        "          #print(token, \"=>\", lemma)\n",
        "          if token != lemma:\n",
        "              if token in new_corpus_line:\n",
        "                  index = new_corpus_line.index(token)\n",
        "                  new_corpus_line[index] = lemma\n",
        "        # Final filter for bad punctuation\n",
        "      bad_punc = [',','','.','-', '\\'', '\"', '’', '”', '“', \">\", '`', ''] #convert to regex when not mad at regex\n",
        "      new_corpus_line = [token for token in new_corpus_line if token not in bad_punc]\n",
        "      ### print(new_corpus_line)\n",
        "\n",
        "      new_corpus_line = ' '.join(new_corpus_line)\n",
        "      return new_corpus_line\n",
        "\n",
        "    #tqdm.pandas()\n",
        "    running_count = 0\n",
        "    tqdm.pandas()\n",
        "    start_len = len(corpus_df)\n",
        "    info_str = f'Starting preprocess on set of size {len(corpus_df)}'\n",
        "    self.pretty_print('START', 'PREPR', info_str)\n",
        "    #print(corpus_df['text'])\n",
        "    #print(corpus_df['label'].value_counts())\n",
        "\n",
        "    corpus_df['text'] = corpus_df['text'].progress_apply(lambda x: preprocess_each_line(x, corpus_df))\n",
        "    corpus_df = corpus_df[corpus_df['text'].str.len() > 0]\n",
        "    corpus_df.reset_index(inplace=True)\n",
        "\n",
        "    # MAYBE REMOVE?\n",
        "    #corpus_df.drop_duplicates()\n",
        "\n",
        "    if exclude_neutral:\n",
        "      corpus_df = corpus_df[corpus_df[\"label\"].str.contains(\"neutral\")==False]\n",
        "\n",
        "    #print('[PREPROCESSING] set after process:')\n",
        "    #print(corpus_df['text'])\n",
        "    #print(corpus_df['label'].value_counts())\n",
        "    end_len = len(corpus_df)\n",
        "    total_removed = start_len - end_len\n",
        "    infostr = f'Preprocessing finished on resulting {end_len} lines, removed total of {total_removed} lines'\n",
        "    self.pretty_print('DONE', 'PREPR', infostr)\n",
        "    return corpus_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#test_str = 'I enjoy family guy'\n",
        "#print(ec.predict_emotion(test_str))\n"
      ],
      "metadata": {
        "id": "H94CDUwY3dFX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1: Identify the emotion label of the comment (excluding neutral)\n",
        "### Instructions for Task 1.1\n",
        "- You will provide a model for multi class classification problem\n",
        "- Model should differentiate between 10 labels, INCLUDING neutral class\n",
        "- Objective: identify which emotion label a text comment represents\n",
        "    - Gold standard label is in the last column in training and test data\n",
        "- **Report accuracy and macro average f1 on later released test set**"
      ],
      "metadata": {
        "id": "I9o4S-sS7v_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test of normal emotion classifier (task 1.1) --> WITH NEUTRALS\n",
        "ec = EmotionClassifier('/content/sample_data/T1_train.csv', 'NONE')\n",
        "ec.train_model(ec.train_corpus_preproccessed)\n",
        "ec.validate_train()\n",
        "ec.print_training_stats()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e304be81955e48d59df2c983a8848ccc",
            "8f62f26f63b549e28c929d4aa07012ac",
            "8b4836bc2ee64586b3de0f5786e92c7a",
            "6ee9b144fbe8412482374daebc2843db",
            "e836b8f2d3c64a2c86e201c63c730d61",
            "7893b4cf88334cf194d97371a6fb40b6",
            "0e3b13e134524c4d9de9edfd07d2a81b",
            "7ad55c38dce24dd1bd64e748f7f971a0",
            "fffbaa3bfd114ffc9a0bcf05b5b88cdf",
            "cb6e19d8cffc47398c68cf6f445ff8e2",
            "3cbf1ea4813f488cbf32d62d0ad50dd7"
          ]
        },
        "id": "vm_dNEXv3c0L",
        "outputId": "ff0a1a40-7696-4d5f-9760-2aeacfe3c44e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - START:INIT] --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - NLTK:INIT] --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus... --: Preforming Pre-processing on train corpus...\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6532 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e304be81955e48d59df2c983a8848ccc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines --: Preprocessing finished on resulting 6490 lines, removed total of 42 lines\n",
            "[EC - DONE:INIT] --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - START:TRAINING] --: Starting training of model\n",
            "[EC - START:TRAINING] --: Starting training of model --: Starting training of model\n",
            "[EC - START:TRAINING] --: Starting training of model --: Starting training of model --: Starting training of model\n",
            "[EC - START:TRAINING] --: Starting training of model --: Starting training of model --: Starting training of model --: Starting training of model\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-0b553cc9686f>:295: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_preprocessed_corpus['length_after_cleaning'] = df_preprocessed_corpus['text'].apply(lambda x: len(x))\n",
            "<ipython-input-22-0b553cc9686f>:296: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_preprocessed_corpus['label_id'] = df_preprocessed_corpus['label'].factorize()[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - START:OVERSAMPLER] --: Starting Oversampling...\n",
            "[EC - DONE:OVERSAMPLER] --: Oversampler complete!\n",
            "[EC - DONE:OVERSAMPLER] --: Oversampler complete! --: Oversampler complete!\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics\n",
            "LinearSVC()\n",
            "[EC - INFO:TRAINING] --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model --: Generating Model --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - DONE:TRAINING] --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete! --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete! --: Training complete! --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete! --: Training complete! --: Training complete! --: Training complete!\n",
            "\t\t -- Preprocessed Training Corpus --\n",
            "      index                                               text       label  \\\n",
            "0         0                                      thank insight   gratitude   \n",
            "1         1  already use ublock thank suggest ghostery didn...   gratitude   \n",
            "2         2                                      thats awesome  admiration   \n",
            "3         3                                               true     neutral   \n",
            "4         4                    also likely sell steal good lol   amusement   \n",
            "...     ...                                                ...         ...   \n",
            "6485   6527             knicks fan chant let go heat sickening     neutral   \n",
            "6486   6528                                             goddam       anger   \n",
            "6487   6529  chart seem indicate approximately net advantag...     neutral   \n",
            "6488   6530                                     thatll cost ya     neutral   \n",
            "6489   6531                       question incentive work hard   curiosity   \n",
            "\n",
            "      length_after_cleaning  label_id  \n",
            "0                        13         0  \n",
            "1                        52         0  \n",
            "2                        13         1  \n",
            "3                         4         2  \n",
            "4                        31         3  \n",
            "...                     ...       ...  \n",
            "6485                     38         2  \n",
            "6486                      6         6  \n",
            "6487                     63         2  \n",
            "6488                     14         2  \n",
            "6489                     28         8  \n",
            "\n",
            "[6490 rows x 5 columns]\n",
            "\n",
            "\n",
            "\t\t -- X_train, Y_train --\n",
            "0                                  thank thats generous xo\n",
            "1        please suggest name marvel next super hero mov...\n",
            "2                              thank kind word thats sweet\n",
            "3                 thank comment thank god wasnt alone feel\n",
            "4                                         thanks apologize\n",
            "                               ...                        \n",
            "20635    thats much engrained problem let let one take ...\n",
            "20636                                          youre stuck\n",
            "20637                            misspelled eastern russia\n",
            "20638                 thats link point directly time stamp\n",
            "20639    manage curb ghg emission would smart enough re...\n",
            "Name: text, Length: 20640, dtype: object\n",
            "0        gratitude\n",
            "1        gratitude\n",
            "2        gratitude\n",
            "3        gratitude\n",
            "4        gratitude\n",
            "           ...    \n",
            "20635      neutral\n",
            "20636      neutral\n",
            "20637      neutral\n",
            "20638      neutral\n",
            "20639      neutral\n",
            "Name: label, Length: 20640, dtype: object\n",
            "\n",
            "\n",
            "\t\t -- x_validate, y_validate --\n",
            "2635                            read article thanks share\n",
            "1391    wanna add perk talk dagger pack get standalone...\n",
            "1326    witness tell police hear suspect scream okay i...\n",
            "3514        nah bunch lot corner make problem everyone el\n",
            "4033                                                thank\n",
            "                              ...                        \n",
            "1362         depend big print probably take hour min hour\n",
            "2701                                           hate china\n",
            "3136                                 guy tell people shut\n",
            "5035                                           cant stand\n",
            "3476                                  thank understanding\n",
            "Name: text, Length: 1298, dtype: object\n",
            "2635    gratitude\n",
            "1391      neutral\n",
            "1326      neutral\n",
            "3514      neutral\n",
            "4033    gratitude\n",
            "          ...    \n",
            "1362      neutral\n",
            "2701    curiosity\n",
            "3136      neutral\n",
            "5035      neutral\n",
            "3476    gratitude\n",
            "Name: label, Length: 1298, dtype: object\n",
            "\t\t -- Validation Results (Class report, Accuracy, etc) --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   gratitude       0.82      0.84      0.83       158\n",
            "  admiration       0.90      0.81      0.85        79\n",
            "     neutral       0.72      0.58      0.65        36\n",
            "   amusement       0.60      0.39      0.48        38\n",
            "    approval       0.41      0.17      0.24        53\n",
            "        love       0.99      0.97      0.98       250\n",
            "       anger       0.90      0.94      0.92       111\n",
            "     sadness       0.79      0.89      0.84       512\n",
            "   curiosity       0.86      0.77      0.81        31\n",
            "    optimism       0.95      0.63      0.76        30\n",
            "\n",
            "    accuracy                           0.84      1298\n",
            "   macro avg       0.79      0.70      0.74      1298\n",
            "weighted avg       0.83      0.84      0.83      1298\n",
            "\n",
            "\t Total Classifications durring validation : 1298\n",
            "\t Incorrect Classifications durring validation : 209\n",
            "\t Total (basic) Classification Accuracy: 83.89830508474576\n",
            "\t 3 examples of classified data results (text, predicted label, actual label):\n",
            "\t ('facility serve free food povertylowincome people tax break etc', 'neutral', 'neutral') \n",
            "\n",
            "\t ('amaze defend matter huh', 'admiration', 'admiration') \n",
            "\n",
            "\t ('dig op ahah video post year ago', 'amusement', 'neutral') \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2: Identify the emotion label of the comment (excluding neutral)\n",
        "### Instructions for Task 1.2\n",
        "- You will provide a model trained for a 9-class classification problem\n",
        "- We will provide a mixed dataset with 10 labels (including neutral).\n",
        "    - Filter out the neutral comments for this task\n",
        "- Objective: Identify which emotion label a text comment represents\n",
        "    - The gold standard reference label is in the last column in the training set and test set.\n",
        "- **Report accuracy and macro-averaged F1 on the later released test set**\n",
        "- You must also ensure your system removes neutral from dataset while testing. The training set and test set files will have the exact same format\n",
        "    "
      ],
      "metadata": {
        "id": "f69ntU5f739q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test of normal emotion classifier (task 1.2) --> WITH OUT NEUTRALS\n",
        "\n",
        "ec_noneutral = EmotionClassifier('/content/sample_data/T1_train.csv', 'NONE', exclude_neutral=True)\n",
        "ec_noneutral.train_model(ec_noneutral.train_corpus_preproccessed)\n",
        "ec_noneutral.validate_train()\n",
        "ec_noneutral.print_training_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "db198ea769c242d1a69250a3dd106d66",
            "9d73f1022d4242de96daf12c483c24b9",
            "a43911feefed426db13221bc2fbc51ea",
            "0e574543aaee4fc09c6846996d654d41",
            "39fc3485ad394ab48c68e5b141f2261f",
            "1806c1f6401b4cbc879fc27cec272f2d",
            "a01cd2af4f1743629a4aabf2e7701304",
            "f8960d909a4d424297ff8351cbf9f352",
            "ab776ba2a6e04789a449203503b24f7f",
            "6ddba80f1a6a47b5ba2233c99c9d9d45",
            "d64cfc1bb74f4fba815966bd0c6db9f4"
          ]
        },
        "id": "BBNc6ao57zjE",
        "outputId": "63ab5ffd-d09e-41a4-8499-de7f88d653c8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - START:INIT] --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - START:INIT] --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier... --: Initalizing Emotion Classifier...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n",
            "[EC - NLTK:INIT] --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data... --: Downloading NLTK Data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - NLTK:INIT] --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - NLTK:INIT] --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete! --: NLTK Data Download Complete!\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - INFO:INIT] --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)... --: Preforming Pre-processing on train corpus (w/o neutral labels)...\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n",
            "[EC - START:PREPR] --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532 --: Starting preprocess on set of size 6532\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6532 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db198ea769c242d1a69250a3dd106d66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:PREPR] --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines --: Preprocessing finished on resulting 3914 lines, removed total of 2618 lines\n",
            "[EC - DONE:INIT] --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - DONE:INIT] --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete --: Initalization complete\n",
            "[EC - START:TRAINING] --: Starting training of model\n",
            "[EC - START:TRAINING] --: Starting training of model --: Starting training of model\n",
            "[EC - START:TRAINING] --: Starting training of model --: Starting training of model --: Starting training of model\n",
            "[EC - START:TRAINING] --: Starting training of model --: Starting training of model --: Starting training of model --: Starting training of model\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf --: Vectorizing features w/tf-idf\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - INFO:TRAINING] --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate --: Splitting Train set into train and validate\n",
            "[EC - START:OVERSAMPLER] --: Starting Oversampling...\n",
            "[EC - DONE:OVERSAMPLER] --: Oversampler complete!\n",
            "[EC - DONE:OVERSAMPLER] --: Oversampler complete! --: Oversampler complete!\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics\n",
            "[EC - INFO:TRAINING] --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics --: Gathering Metrics\n",
            "LinearSVC()\n",
            "[EC - INFO:TRAINING] --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Generating Model --: Generating Model --: Generating Model --: Generating Model --: Generating Model\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - INFO:TRAINING] --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated --: Model Generation Finished and classreport generated\n",
            "[EC - DONE:TRAINING] --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete! --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete! --: Training complete! --: Training complete!\n",
            "[EC - DONE:TRAINING] --: Training complete! --: Training complete! --: Training complete! --: Training complete! --: Training complete!\n",
            "\t\t -- Preprocessed Training Corpus --\n",
            "      index                                               text       label  \\\n",
            "0         0                                      thank insight   gratitude   \n",
            "1         1  already use ublock thank suggest ghostery didn...   gratitude   \n",
            "2         2                                      thats awesome  admiration   \n",
            "4         4                    also likely sell steal good lol   amusement   \n",
            "6         6                            absolutely gorgeous pas  admiration   \n",
            "...     ...                                                ...         ...   \n",
            "6481   6523  purdues bad loss notre dame kenpom new mexico ...  admiration   \n",
            "6482   6524  im painfully aware buy freesync monitor nvidia...     sadness   \n",
            "6484   6526  agree last paragraph isnt fault side didnt wan...    approval   \n",
            "6486   6528                                             goddam       anger   \n",
            "6489   6531                       question incentive work hard   curiosity   \n",
            "\n",
            "      length_after_cleaning  label_id  \n",
            "0                        13         0  \n",
            "1                        52         0  \n",
            "2                        13         1  \n",
            "4                        31         2  \n",
            "6                        23         1  \n",
            "...                     ...       ...  \n",
            "6481                     65         1  \n",
            "6482                     59         6  \n",
            "6484                     66         3  \n",
            "6486                      6         5  \n",
            "6489                     28         7  \n",
            "\n",
            "[3914 rows x 5 columns]\n",
            "\n",
            "\n",
            "\t\t -- X_train, Y_train --\n",
            "0                                                   great\n",
            "1       thats awesomesometimes get shower seem like im...\n",
            "2                                        think look great\n",
            "3                                        floor nice shiny\n",
            "4                                                    like\n",
            "                              ...                        \n",
            "9265                                        youre welcome\n",
            "9266                                       thank tc get u\n",
            "9267       best way ship item roughly x x weigh lbs thank\n",
            "9268    im university student even though job im go fi...\n",
            "9269    wow definitely need im glad stumble onto comme...\n",
            "Name: text, Length: 9270, dtype: object\n",
            "0       admiration\n",
            "1       admiration\n",
            "2       admiration\n",
            "3       admiration\n",
            "4       admiration\n",
            "           ...    \n",
            "9265     gratitude\n",
            "9266     gratitude\n",
            "9267     gratitude\n",
            "9268     gratitude\n",
            "9269     gratitude\n",
            "Name: label, Length: 9270, dtype: object\n",
            "\n",
            "\n",
            "\t\t -- x_validate, y_validate --\n",
            "4547                  would funny guy get hit car end tho\n",
            "320                                        subscribe love\n",
            "3445                                      thanks response\n",
            "4114                                         thank remind\n",
            "1020                                             ah thank\n",
            "                              ...                        \n",
            "2156    couldve write last part first part serve poten...\n",
            "1304    insightful tragic post op thank share raise aw...\n",
            "5517                                     thanks good news\n",
            "4771    endorse one epic fail regard prefer make infor...\n",
            "1316    yeah never fight im glad manage end good term ...\n",
            "Name: text, Length: 783, dtype: object\n",
            "4547    amusement\n",
            "320          love\n",
            "3445    gratitude\n",
            "4114    gratitude\n",
            "1020    gratitude\n",
            "          ...    \n",
            "2156    gratitude\n",
            "1304    gratitude\n",
            "5517    gratitude\n",
            "4771    gratitude\n",
            "1316    gratitude\n",
            "Name: label, Length: 783, dtype: object\n",
            "\t\t -- Validation Results (Class report, Accuracy, etc) --\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   gratitude       0.82      0.84      0.83       179\n",
            "  admiration       0.83      0.91      0.87        70\n",
            "   amusement       0.80      0.70      0.75        40\n",
            "    approval       0.59      0.59      0.59        44\n",
            "        love       0.50      0.47      0.48        58\n",
            "       anger       0.99      0.98      0.98       225\n",
            "     sadness       0.92      0.93      0.92        98\n",
            "   curiosity       0.85      0.91      0.88        32\n",
            "    optimism       0.81      0.70      0.75        37\n",
            "\n",
            "    accuracy                           0.85       783\n",
            "   macro avg       0.79      0.78      0.78       783\n",
            "weighted avg       0.84      0.85      0.84       783\n",
            "\n",
            "\t Total Classifications durring validation : 783\n",
            "\t Incorrect Classifications durring validation : 121\n",
            "\t Total (basic) Classification Accuracy: 84.54661558109834\n",
            "\t 3 examples of classified data results (text, predicted label, actual label):\n",
            "\t ('dat beauty get sniped auction second leave lose summer', 'admiration', 'curiosity') \n",
            "\n",
            "\t ('thank thought say well', 'gratitude', 'gratitude') \n",
            "\n",
            "\t ('excelerate look really good yo theyve outslayed every map execute perfect play fun play full potential', 'admiration', 'admiration') \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pickling of data for future use\n",
        "### Operation will pickle entire emotionclassifier object, which will be 2 with and without neutrals respectivly"
      ],
      "metadata": {
        "id": "ZKO30kXj9HTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PICKLING OF OBJECT --> for future use, as we cannot change our model after tuesday\n",
        "# saves as two files, EmotionClassifierObject (with neutrals), and EmotionClassifierNoNeutralsObject (w/o neutrals)\n",
        "\n",
        "#'''\n",
        "pickle_object_save(ec, 'EmotionClassifierObject.pkl')\n",
        "pickle_object_save(ec, 'EmotionClassifierNoNeutralsObject.pkl')\n",
        "#'''\n",
        "\n",
        "# how to grab pickle object\n",
        "#'''\n",
        "EmotionClassifierObject = None\n",
        "with open('EmotionClassifierObject.pkl', 'rb') as inp:\n",
        "  EmotionClassifierObject = pickle.load(inp)\n",
        "\n",
        "print(EmotionClassifierObject.predict_emotion('I love flowers'))\n",
        "# output: ['anger']\n",
        "#'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGPc3QLQ8FZy",
        "outputId": "125f03af-265f-47c3-9bbe-88e5570de2d4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMOTION CLASSIFIER"
      ],
      "metadata": {
        "id": "Gl4k6Xbx9QDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test implementation when test file given\n",
        "# this is a FAKE test file, just parts of our train data (results inaccurate from this, just to test func)\n",
        "# Add your own file\n",
        "#\"\"\"\n",
        "EmotionClassifierObject.preprocess_test_corpus('add_your_own.csv')\n",
        "EmotionClassifierObject.test_model()\n",
        "#\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "TtZLMKev9LHB",
        "outputId": "8d496e36-e38a-4244-85c1-0451567fb07d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-4aacf000d58c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mEmotionClassifierObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_test_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'T1_testFAKE.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mEmotionClassifierObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-0b553cc9686f>\u001b[0m in \u001b[0;36mpreprocess_test_corpus\u001b[0;34m(self, test_filename, exclude_neutral)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude_neutral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'T1_testFAKE.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WQsRMrH59UZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}